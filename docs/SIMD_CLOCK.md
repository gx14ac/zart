## CPU Clocks
AI時代だからこそ、システムプログラミングを重要視していこう。
私は可読性を多少犠牲にできるといったことを伝えたい。

SIMDはプログラムの高速化を図る上で重要な命令セットである。
例えば5.0GHzのCPUがあった場合は1秒間に
50億クロック動作する。
20コアの場合だと20倍で1000億クロック動くことになる。
 - 5.0GHz = 1秒間に50億クロック（1GHz = 10億クロック）
 - 20コア = そのクロック数が20倍

初学者のためにクロックとは何か説明しておくと
クロック（clock）は、コンピュータのCPUや電子回路が「いつ処理を行うか」を決めるためのリズムや拍子のようなものです。
このクロック信号が1回発生するごとに、CPUは命令を1つ実行したり、データをやり取りしたりします。
つまり1秒間で1000億回クロックできる。

この処理をもっとうまく扱うべきだと考えた私。

特にRunetaleやシステムプログラミングを行っている方は意識すると想いもよらないシステムの最適化ができる。

**AVXやAVX2のような256bitのSIMD命令ではint型やfloat型を8個同時に扱える。**
理想的には処理速度が8倍、実際の使用場面でも処理速度が4,5倍程度にはなるように思えます。
しかし実際には大抵高速化の効果が1.5～2倍程度で頭打ちし、全く性能を引き出せません。

つまりこれにAVX2を適用すれば、理想的にはint型に対し毎秒
**1000億回×8=8000億回の処理が行えます。**

しかし問題はメモリにあるようで
- 3200MHz：メモリの動作クロック（1秒間に3200万回のデータ転送）
- 64bit：1回の転送で送れるデータ量（バス幅、1バイト=8ビットなので64ビット=8バイト）
- 2：デュアルチャネル（2本のメモリが同時に動作）

としたメモリの場合に

 1. 1チャネルあたりの帯域幅を計算
3200MHz × 64bit = 204,800Mbit/秒
 2. ビットをバイトに変換
204,800Mbit ÷ 8 = 25,600MB/秒（1バイト=8ビット）
 3. チャネル数をかける
25,600MB/秒 × 2 = 51,200MB/秒 = 51.2GB/秒

 **3200MHz × 64bit × 2 ÷ 8 = 51,200MB/s = 51.2GB/s**
 といった具合になる

このメモリ帯域幅がint型１つ4バイトを毎秒処理したとすると、
**「51.2GB/sの帯域幅があれば、4バイトのint型を毎秒128億個転送できる」という意味です。**

 1. メモリ帯域幅をバイト単位に直す
51.2GB/s = 51,200,000,000バイト/秒
 2. int型1個のサイズを確認
int型 = 4バイト
 3. 帯域幅をint型のサイズで割る
51,200,000,000バイト ÷ 4バイト = 12,800,000,000個/秒
 4. 12,800,000,000個/秒 = 128億個/秒

この部分が表していることは
**「CPUの計算能力がいくら高くても、メモリからデータを受け取る速度（メモリ帯域幅）が遅いと、結局CPUが待たされてしまい、全体の処理速度が上がらない」**
という現象を説明しています。

これを**メモリボトルネック問題**と称します。

SIMD命令初心者向けに書かれたコードで速度が4,5倍になった！というのも時折見かけますが、その大抵の理由はシングルスレッドコードだからです。

マルチスレッド且つSIMD命令を適切に扱って、計算処理をメモリのボトルネック解消しながらコーディングを行う必要があります。


## メモリアクセスを控える
現代のCPUでは1コアあたりにL2キャッシュが1MBくらいある。
この場合
 1. キャッシュ容量をバイトに直す
1MB = 1,048,576バイト
 2. int型1個のサイズ
4バイト
 3. キャッシュに入るint型の個数
1,048,576バイト ÷ 4バイト = 262,144個

L3キャッシュは平均16MB～64MB程度、仮に64MBのキャッシュがあったとすれば
16,000,000バイトあるので、
16,000,000バイト ÷ 4バイト = 4,000,000個

となり400万個近くの正数データをキャッシュできる。

例えばこのように1億個のデータを扱うコードがあったとする。
```c++
  for(int i=0; i<100000000; i++){
    (処理A);
  }

  (処理X);

  for(int i=0; i<100000000; i++){
    (処理B);
  }

  (処理Y);

  for(int i=0; i<100000000; i++){
    (処理C);
  }
```

先ほどの説明だとCPUきゃっすは合計しても数十MB程度なので
 - L2キャッシュ：1MB（=1,048,576バイト）
 - L3キャッシュ：数MB～数十MB（例：16MB～64MB）

 扱うデータ：400MB（=400,000,000バイト）
 - int型1個 = 4バイト
 - 1億個 = 400,000,000バイト = 400MB

なので当然キャッシュには収まることなく、メモリアクセスを許してしまうため、メモリボトルネックの問題が発生してしまう。

では先ほどのキャッシュ最適化前のコードを最適化してみると
```c++
// キャッシュ最適化前
#include <vector>
#include <iostream>

int main() {
    const int N = 100000000; // 1億
    std::vector<int> A(N, 1), B(N, 2), C(N, 3), D(N);

    // (処理A): 例としてA+B
    for (int i = 0; i < N; i++) {
        D[i] = A[i] + B[i];
    }

    // (処理X): 何らかの処理
    int sum = 0;
    for (int i = 0; i < N; i++) {
        sum += D[i];
    }

    // (処理B): 例としてD+C
    for (int i = 0; i < N; i++) {
        D[i] = D[i] + C[i];
    }

    // (処理Y): 何らかの処理
    int sum2 = 0;
    for (int i = 0; i < N; i++) {
        sum2 += D[i];
    }

    // (処理C): 例としてD*2
    for (int i = 0; i < N; i++) {
        D[i] = D[i] * 2;
    }

    std::cout << sum << " " << sum2 << " " << D[0] << std::endl;
    return 0;
}
```

このように最適化ができます。
```c++
// キャッシュ最適化後（ループ分割）
#include <vector>
#include <iostream>

int main() {
    const int N = 100000000; // 1億
    const int BLOCK = 10000; // 1万
    std::vector<int> A(N, 1), B(N, 2), C(N, 3), D(N);

    int sum = 0, sum2 = 0;

    for (int j = 0; j < N / BLOCK; j++) {
        int start = j * BLOCK;
        int end = start + BLOCK;

        // (処理A'): A+B
        for (int k = start; k < end; k++) {
            D[k] = A[k] + B[k];
        }

        // (処理X'): 何らかの処理
        for (int k = start; k < end; k++) {
            sum += D[k];
        }

        // (処理B'): D+C
        for (int k = start; k < end; k++) {
            D[k] = D[k] + C[k];
        }

        // (処理Y'): 何らかの処理
        for (int k = start; k < end; k++) {
            sum2 += D[k];
        }

        // (処理C'): D*2
        for (int k = start; k < end; k++) {
            D[k] = D[k] * 2;
        }
    }

    std::cout << sum << " " << sum2 << " " << D[0] << std::endl;
    return 0;
}
```

どういうことか？
- 元のコードは1億個（400MB）のデータを一気に処理しようとします。これはキャッシュに全く収まりません。
- 書き換え後は、外側のループ（j）が1万回、内側のループ（k）が1万回で、内側のループ1回ごとに1万個（4バイト×1万=40KB）のデータだけを処理します。

最適化後、なぜキャッシュに収まるのか ￼
- 1万個 × 4バイト = 40,000バイト = 40KB
- 40KBは、L1やL2キャッシュ（数十KB～1MB）に十分収まるサイズです。
- つまり、**「今必要なデータだけ」**をキャッシュに乗せて高速に処理できるようになる。

このメリット
- 毎回キャッシュに収まる範囲だけを処理するので、メインメモリへのアクセスが激減し、CPUの高速なキャッシュを最大限活用できます。
- これにより、理論値に近い高速化（SIMD命令の効果も最大限発揮）が期待できます。

## 次に考えるべきはレジスタ
https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html

レジスタにデータを置いたまま計算を行えるなら、メモリアクセスは最小限で済みます。具体例としてベクトルデータA,B,Cの和をとることを考えると
```asm
  D = _mm256_add_epi32(A,B);
  E = _mm256_add_epi32(D,C);
```
Dの結果が入らない場合はさらに省略化してかけるため、このようにかける。
```asm
  E = _mm256_add_epi32(_mm256_add_epi32(A,B),C);
```

命令をまとめて1行に書くことで、最適化されやすいコードを書けるが
現代のプログラミングにおいては最適化より可読性を加味したコードの方が好まれやすい。
見づらくなることを承知で書くことも1つの選択肢になってくる。
しかしLLMの登場でコードの書き方も一定変わるとは思っていて、
このように最適化コードを書いても、チームが内容を理解していればわざわざ可読性を高めたコードを書かなくても保守運用はできるはず。

## ZigにおけるSIMD
https://pedropark99.github.io/zig-book/Chapters/15-vectors.html

## データを無駄なく使う
データの無駄遣いになっている例としてはbool型があります。bool型は2値変数なので1bitあれば格納できるのですが、原則1byte=8bit分使って格納されています。そのため必要量の8倍のメモリを使っており、転送する際には必要量の8倍の帯域を食っています。ここまで露骨なものは稀ですが、例えばデータに対してand演算などでマスク処理をする場合、マスクを掛けなかった部分のデータは捨てられてしまうわけです。この捨てられた分も当然帯域幅を食っているので、もし処理方法を工夫して最初からこの部分を削って転送できたならば、処理速度を上げられます。

## 極力単純な命令を使う
SIMDには複雑な命令セットが多数存在する。
同じ計算であっても違う関数群を用いて2通り以上で表現できる場合がある。
そういう時はなるべく単純な関数を多用しているものを選ぶ というのが基本です。これはレイテンシを抑えることを目的としている。
ここでレイテンシとスループットの2つの用語について軽く解説しておきます。
レイテンシとは命令が発行されてから計算が完了するまでのクロック数、スループットは同時に発行できる命令数の逆数です。これらはいずれも小さい方が処理速度が速いです。

一般的に加算やbit演算などの基本的なSIMD演算におけるレイテンシ・スループットはかなり低く、反対に丸め処理などの少し特殊な演算では大きくなります。 よって基本的な演算を多く利用するよう意識することで早く処理してくれる可能性が上がります。

